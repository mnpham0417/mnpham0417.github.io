---
---

@article{
abbr={Preprint},
  title={Circumventing Concept Erasure Methods For Text-to-Image Generative Models},
  author={Minh Pham and Kelly O. Marshall and Chinmay Hegde},
  journal={arXiv preprint arXiv:2308.01508},
  pdf={https://arxiv.org/pdf/2308.01508.pdf},
  abstract={Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to "erase" sensitive concepts from text-to-image models. In this work, we examine five recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve "erased" concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.},
  year={2023}
}



@article{pham2022revisiting,
abbr = {Preprint},
  title={Revisiting self-distillation},
  author={Minh Pham* and Minsu Cho* and Ameya Joshi* and and Chinmay Hegde},
  journal={arXiv preprint arXiv:2206.08491},
  pdf={https://arxiv.org/pdf/2206.08491.pdf},
  abstract={Knowledge distillation is the procedure of transferring "knowledge" from a large model (the teacher) to a more compact one (the student), often being used in the context of model compression. When both models have the same architecture, this procedure is called self-distillation. Several works have anecdotally shown that a self-distilled student can outperform the teacher on held-out data. In this work, we systematically study self-distillation in a number of settings. We first show that even with a highly accurate teacher, self-distillation allows a student to surpass the teacher in all cases. Secondly, we revisit existing theoretical explanations of (self) distillation and identify contradicting examples, revealing possible drawbacks of these explanations. Finally, we provide an alternative explanation for the dynamics of self-distillation through the lens of loss landscape geometry. We conduct extensive experiments to show that self-distillation leads to flatter minima, thereby resulting in better generalization.},
  year={2022}
}

@article{joshi2022smooth,
abbr = {Preprint},
  title={Smooth-Reduce: Leveraging Patches for Improved Certified Robustness},
  author={Ameya Joshi and Minh Pham and Minsu Cho and Leonid Boytsov and Filipe Condessa and J. Zico Kolter and and Chinmay Hegde},
  journal={arXiv preprint arXiv:2205.06154},
  pdf={https://arxiv.org/pdf/2205.06154.pdf},
  abstract={Randomized smoothing (RS) has been shown to be a fast, scalable technique for certifying the robustness of deep neural network classifiers. However, methods based on RS require augmenting data with large amounts of noise, which leads to significant drops in accuracy. We propose a training-free, modified smoothing approach, Smooth-Reduce, that leverages patching and aggregation to provide improved classifier certificates. Our algorithm classifies overlapping patches extracted from an input image, and aggregates the predicted logits to certify a larger radius around the input. We study two aggregation schemes -- max and mean -- and show that both approaches provide better certificates in terms of certified accuracy, average certified radii and abstention rates as compared to concurrent approaches. We also provide theoretical guarantees for such certificates, and empirically show significant improvements over other randomized smoothing methods that require expensive retraining. Further, we extend our approach to videos and provide meaningful certificates for video classifiers.},
  year={2022}
}

@article{pham2020tbse,
abbr = {ICASSP},
  title={Toward better speaker embeddings: Automated collection of speech samples from unknown distinct speakers},
  author={Minh Pham and Zeqian Li and and Jacob Whitehill},
  journal={IEEE International Conference on Acoustics, Speech and Signal Processing},
  year={2020},
  published={true},
  selected = {true},
  pdf={https://par.nsf.gov/servlets/purl/10158092},
  abstract={The accuracy of speaker verification and diarization models depends on the quality of the speaker embeddings used to separate audio samples from different speakers. With the goal of training better embedding models, we devise an au- tomatic pipeline for large-scale collection of speech samples from unique speakers that is significantly more automated than previous approaches. With this pipeline, we collect and publish the BookTubeSpeech dataset, containing 8,450 YouTube videos (7.74 min per video on average) that each contains a single unique speaker. Using this dataset combined with VoxCeleb2, we show a substantial improvement in the quality of embeddings when tested on LibriSpeech compared to a model trained on only VoxCeleb2.}
  }

@article{pham2020rne,
abbr = {INTERSPEECH},
  title={How does label noise affect the quality of speaker embeddings?},
  author={Minh Pham and Zeqian Li and and Jacob Whitehill},
  journal={Conference of the International Speech Communication Association},
  year={2020},
  published={true},
  selected = {true},
  pdf={http://www.interspeech2020.org/uploadfile/pdf/Wed-3-5-2.pdf},
  abstract={A common assumption when collecting speech datasets is that the accuracy of data labels strongly influences the accuracy of speaker embedding models and verification systems trained from these data. However, we show in experiments1 on the large and diverse VoxCeleb2 dataset that this is not always the case: Under four different labeling models (Split, Merge, Permute, and Corrupt), we find that the impact on trained speaker em- bedding models, as measured by the Equal Error Rate (EER) of speaker verification, is mild (just a few percent absolute error increase), except with very large amounts of noise (i.e., every minibatch is almost completely corrupted). This suggests that efforts to collect speech datasets might benefit more from en- suring large size and diversity rather than meticulous labeling.}
  }

